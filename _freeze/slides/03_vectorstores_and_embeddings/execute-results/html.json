{
  "hash": "42d81a533cec098427fa1a519905c342",
  "result": {
    "markdown": "---\ntitle: Vectorstores and Embeddings\ntitle-slide-attributes:\n  data-background-image: ../images/logo.png\n  data-background-size: contain\n  data-background-opacity: \"0.5\"\nlang: en\nsubtitle: LangChain Tutorial 3\nauthor: Jan Kirenz\nexecute:\n  eval: false\n  echo: true\nhighlight-style: github\nformat:\n  revealjs: \n    toc: true\n    toc-depth: 1\n    embed-resources: false\n    theme: [dark, ../custom.scss]  \n    incremental: true\n    transition: slide\n    background-transition: fade\n    transition-speed: slow\n    code-copy: true\n    code-line-numbers: true\n    smaller: false\n    scrollable: true\n    slide-number: c\n    preview-links: auto\n    chalkboard: \n      buttons: false\n   # logo: ../images/logo.png\n    footer: Jan Kirenz\n---\n\n# Vectorstores and Embeddings\n\nDive into the concept of embeddings and explore vector store integrations within LangChain.\n\n# Setup\n\n## Python\n\n::: {.cell height='166' tags='[]' execution_count=1}\n``` {.python .cell-code}\nimport os\nimport numpy as np\nimport openai\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\n\n\n#import sys\n#sys.path.append('../..')\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\n\nopenai.api_key  = os.environ['OPENAI_API_KEY']\n```\n:::\n\n\n# Loading and Splitting\n\n## Load data\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Load PDF\nloaders = [\n    # Duplicate documents on purpose - messy data\n    PyPDFLoader(\"../docs/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n    PyPDFLoader(\"../docs/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n    PyPDFLoader(\"../docs/cs229_lectures/MachineLearning-Lecture02.pdf\"),\n    PyPDFLoader(\"../docs/cs229_lectures/MachineLearning-Lecture03.pdf\")\n]\n\ndocs = []\nfor loader in loaders:\n    docs.extend(loader.load())\n```\n:::\n\n\n## Define splitter \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = 1500,\n    chunk_overlap = 150\n)\n```\n:::\n\n\n## Split data\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nsplits = text_splitter.split_documents(docs)\n```\n:::\n\n\n. . .\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nlen(splits)\n```\n:::\n\n\n- 209\n\n# Embeddings\n\n## OpenAIEmbeddings\n\n- Let's take our splits and embed them.\n\n. . .\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nembedding = OpenAIEmbeddings()\n```\n:::\n\n\n## Examples\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nsentence1 = \"i like dogs\"\nsentence2 = \"i like canines\"\nsentence3 = \"the weather is ugly outside\"\n```\n:::\n\n\n. . .\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nembedding1 = embedding.embed_query(sentence1)\nembedding2 = embedding.embed_query(sentence2)\nembedding3 = embedding.embed_query(sentence3)\n```\n:::\n\n\n## Compare similarity\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nnp.dot(embedding1, embedding2)\n```\n:::\n\n\n- 0.9631851837941705\n\n. . .\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nnp.dot(embedding1, embedding3)\n```\n:::\n\n\n- 0.7710851013557284\n\n. . .\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nnp.dot(embedding2, embedding3)\n```\n:::\n\n\n- 0.7596334120325541\n\n# Chroma Vectorstore\n\n## Setup\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\npersist_directory = '../docs/chroma/'\n```\n:::\n\n\n. . .\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n!rm -rf ../docs/chroma  # remove old database files if any\n```\n:::\n\n\n## Store data\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nvectordb = Chroma.from_documents(\n    documents=splits,\n    embedding=embedding,\n    persist_directory=persist_directory\n)\n```\n:::\n\n\n## Inspect data\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nprint(vectordb._collection.count())\n```\n:::\n\n\n- 209\n\n# Similarity Search\n\n## Search vectorstore for email\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nquestion = \"is there an email i can ask for help\"\n```\n:::\n\n\n. . .\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ndocs = vectordb.similarity_search(question,k=3)\n```\n:::\n\n\n. . .\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nlen(docs)\n```\n:::\n\n\n- 3\n\n## Inspect data\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\ndocs[0].page_content\n```\n:::\n\n\n- \"cs229-qa@cs.stanford.edu. This goes to an acc ount that's read by all the TAs and me. So \\nrather than sending us email individually, if you send email to this account, it will \\nactually let us get back to you maximally quickly with answers to your questions.  \\nIf you're asking questions about homework probl ems, please say in the subject line which \\nassignment and which question the email refers to, since that will also help us to route \\nyour question to the appropriate TA or to me  appropriately and get the response back to \\nyou quickly.  \\nLet's see. Skipping ahead — let's see — for homework, one midterm, one open and term \\nproject. Notice on the honor code. So one thi ng that I think will help you to succeed and \\ndo well in this class and even help you to enjoy this cla ss more is if you form a study \\ngroup.  \\nSo start looking around where you' re sitting now or at the end of class today, mingle a \\nlittle bit and get to know your classmates. I strongly encourage you to form study groups \\nand sort of have a group of people to study with and have a group of your fellow students \\nto talk over these concepts with. You can also  post on the class news group if you want to \\nuse that to try to form a study group.  \\nBut some of the problems sets in this cla ss are reasonably difficult.  People that have \\ntaken the class before may tell you they were very difficult. And just I bet it would be \\nmore fun for you, and you'd probably have a be tter learning experience if you form a\"\n\n\n## Persist data\n\n- Let's save this so we can use it later!\n\n. . .\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nvectordb.persist()\n```\n:::\n\n\n# Failure modes\n\n## Basics\n\n- This seems great, and basic similarity search will get you 80% of the way there very easily. \n\n- But there are some failure modes that can creep up. \n\n- Here are some edge cases that can arise \n\n## Search vectorstore for matlab\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nquestion = \"what did they say about matlab?\"\n```\n:::\n\n\n. . .\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ndocs = vectordb.similarity_search(question,k=5)\n```\n:::\n\n\n## Inspect data {.smaller}\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\ndocs[0]\n```\n:::\n\n\n- Document(page_content='those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn\\'t.  \\nSo I guess for those of you that haven\\'t s een MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it\\'s sort of an extremely easy to  learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your  own home computer or something if you \\ndon\\'t have a MATLAB license, for the purposes of  this class, there\\'s also — [inaudible] \\nwrite that down [inaudible] MATLAB — there\\' s also a software package called Octave \\nthat you can download for free off the Internet. And it has somewhat fewer features than MATLAB, but it\\'s free, and for the purposes of  this class, it will work for just about \\neverything.  \\nSo actually I, well, so yeah, just a side comment for those of you that haven\\'t seen \\nMATLAB before I guess, once a colleague of mine at a different university, not at \\nStanford, actually teaches another machine l earning course. He\\'s taught it for many years. \\nSo one day, he was in his office, and an old student of his from, lik e, ten years ago came \\ninto his office and he said, \"Oh, professo r, professor, thank you so much for your', metadata={'source': '../docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 8})\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\ndocs[1]\n```\n:::\n\n\n- Document(page_content='those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn\\'t.  \\nSo I guess for those of you that haven\\'t s een MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it\\'s sort of an extremely easy to  learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your  own home computer or something if you \\ndon\\'t have a MATLAB license, for the purposes of  this class, there\\'s also — [inaudible] \\nwrite that down [inaudible] MATLAB — there\\' s also a software package called Octave \\nthat you can download for free off the Internet. And it has somewhat fewer features than MATLAB, but it\\'s free, and for the purposes of  this class, it will work for just about \\neverything.  \\nSo actually I, well, so yeah, just a side comment for those of you that haven\\'t seen \\nMATLAB before I guess, once a colleague of mine at a different university, not at \\nStanford, actually teaches another machine l earning course. He\\'s taught it for many years. \\nSo one day, he was in his office, and an old student of his from, lik e, ten years ago came \\ninto his office and he said, \"Oh, professo r, professor, thank you so much for your', metadata={'source': '../docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 8})\n\n## Insights\n\n- Notice that we're getting duplicate chunks (because of the duplicate `MachineLearning-Lecture01.pdf` in the index).\n\n- Semantic search fetches all similar documents, but does not enforce diversity.\n\n- `docs[0]` and `docs[1]` are indentical.\n\n## Search vectorstore for third lecture\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nquestion = \"what did they say about regression in the third lecture?\"\n```\n:::\n\n\n. . .\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\ndocs = vectordb.similarity_search(question,k=5)\n```\n:::\n\n\n## Inspect data {.smaller}\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nfor doc in docs:\n    print(doc.metadata)\n```\n:::\n\n\n. . .\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\n{'source': '../docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 0}\n{'source': '../docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 14}\n{'source': '../docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 0}\n{'source': '../docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 6}\n{'source': '../docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 8}\n```\n:::\n\n\n- The question was about the third lecture, but includes results from other lectures as well.\n\n- We discuss approaches to handle these problems in the next tutorial \n\n\n# Acknowledgments\n\n- This tutorial is mainly based on the excellent course [\"LangChain: Chat with Your DataI\"](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/) provided by Harrison Chase from LangChain and Andrew Ng from DeepLearning.AI.\n\n\n# What's next? {background-image=\"../images/logo.png\" background-opacity=\"0.5\"}\n\n**Congratulations! You have completed this tutorial** 👍\n\n\n**Next, you may want to go back to the [lab's website](https://kirenz.github.io/lab-langchain-rag/)**\n\n",
    "supporting": [
      "03_vectorstores_and_embeddings_files"
    ],
    "filters": [],
    "includes": {}
  }
}