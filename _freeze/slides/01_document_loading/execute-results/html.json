{
  "hash": "29008bc98ddce98f3a6bd6307f680c0b",
  "result": {
    "markdown": "---\ntitle: Document Loading\ntitle-slide-attributes:\n  data-background-image: ../images/logo.png\n  data-background-size: contain\n  data-background-opacity: \"0.5\"\nlang: en\nsubtitle: LangChain Tutorial 1\nauthor: Jan Kirenz\nexecute:\n  eval: false\n  echo: true\nhighlight-style: github\nformat:\n  revealjs: \n    toc: true\n    toc-depth: 1\n    embed-resources: false\n    theme: [dark, ../custom.scss]  \n    incremental: true\n    transition: slide\n    background-transition: fade\n    transition-speed: slow\n    code-copy: true\n    code-line-numbers: true\n    smaller: false\n    scrollable: true\n    slide-number: c\n    preview-links: auto\n    chalkboard: \n      buttons: false\n   # logo: ../images/logo.png\n    footer: Jan Kirenz\n---\n\n# Document Loading\n\nLearn the fundamentals of data loading and discover over 80 unique loaders LangChain provides to access diverse data sources, including audio and video.\n\n# Setup\n\n## Python\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom langchain.document_loaders import NotionDirectoryLoader\nfrom langchain.document_loaders import WebBaseLoader\nimport pandas as pd\nfrom langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\nfrom langchain.document_loaders.parsers import OpenAIWhisperParser\nfrom langchain.document_loaders.generic import GenericLoader\nfrom langchain.document_loaders import PyPDFLoader\nfrom dotenv import load_dotenv, find_dotenv\nimport os\nimport openai\n# import sys\n# sys.path.append('../..')\n\n_ = load_dotenv(find_dotenv())  # read local .env file\n\nopenai.api_key = os.environ['OPENAI_API_KEY']\n```\n:::\n\n\n# Retrieval Augmented Generation (RAG)\n \n## Basics\n\n- In retrieval augmented generation (RAG), an LLM retrieves contextual documents from an external dataset as part of its execution. \n\n- This is useful if we want to ask question about specific documents (e.g., our PDFs, a set of videos, etc). \n\n![](/images/rag.png)\n\n\n# PDF \n\n## Example\n\n- Let's load a PDF [transcript](https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture01.pdf) from one of Andrew Ng's courses\n\n- These documents are the result of automated transcription so words and sentences are sometimes split unexpectedly.\n\n## Load PDF\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nloader = PyPDFLoader(\"../docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\npages = loader.load()\n```\n:::\n\n\n- Each page is a `Document`.\n\n- A `Document` contains text (`page_content`) and `metadata`.\n\n## Inspect data \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nlen(pages)\n```\n:::\n\n\n- 22\n\n. . .\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\npage = pages[0]\n```\n:::\n\n\n. . .\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\npage.metadata\n```\n:::\n\n\n- {'source': '../docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0}\n\n\n## Inspect content {.smaller}\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nprint(page.page_content[0:500])\n```\n:::\n\n\n- MachineLearning-Lecture01  \nInstructor (Andrew Ng):  Okay. Good morning. Welcome to CS229, the machine \nlearning class. So what I wanna do today is ju st spend a little time going over the logistics \nof the class, and then we'll start to  talk a bit about machine learning.  \nBy way of introduction, my name's  Andrew Ng and I'll be instru ctor for this class. And so \nI personally work in machine learning, and I' ve worked on it for about 15 years now, and \nI actually think that machine learning i\n\n# YouTube\n\n## Prerequisites\n\n- You need [FFmpeg](https://ffmpeg.org/) \n\n- Mac: [install with Homebrew](https://formulae.brew.sh/formula/ffmpeg)\n\n## Example\n\nLet's load the \"Code Report\" about Vector databases from Fireship\n\n\n <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/klTvEwg3oJ4?si=VPuxdiw9QaWfIqbD\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe> \n\n\n## Load YouTube video\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# link to video\nurl = \"https://www.youtube.com/watch?v=klTvEwg3oJ4\"\n\n# path to directory\nsave_dir = \"../docs/youtube/\"\n\n# load video\nloader = GenericLoader(\n    YoutubeAudioLoader([url], save_dir),\n    OpenAIWhisperParser()\n)\n\ndocs = loader.load()\n```\n:::\n\n\n## Inspect data\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndocs[0].page_content[0:500]\n```\n:::\n\n\n- \"It is April 7th, 2023, and you're watching The Code Report. One month ago, Vector Database Weaviate landed $16 million in Series A funding. Last week, PineconeDB just got a check for $28 million at a $700 million valuation. And yesterday, Chroma, an open source project with only 1.2 GitHub stars, raised $18 million for its Embeddings database. And I just launched my own Vector database this morning. We're currently pre-revenue, pre-vision, and pre-code, and valued at $420 million. Leave your cre\"\n\n\n## Save data\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndf = pd.DataFrame(docs, columns=['Text', 'Metadata'])\n```\n:::\n\n\n. . .\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ndf.to_csv('../docs/youtube/codereport.csv')\n```\n:::\n\n\n# URLs\n\n## Example\n\n- Let's load a page from \"Introduction to Modern Statistics\" by Mine √áetinkaya-Rundel and Johanna Hardin: <https://openintro-ims.netlify.app/data-design>\n\n- The raw file is provided in GutHub under this URL: <https://raw.githubusercontent.com/OpenIntroStat/ims/main/02-data-design.qmd>\n\n## Load URL\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nloader = WebBaseLoader(\n    \"https://raw.githubusercontent.com/OpenIntroStat/ims/main/02-data-design.qmd\")\n\ndocs = loader.load()\n```\n:::\n\n\n## Inspact data\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nprint(docs[0].page_content[400:800])\n```\n:::\n\n\n- ampling. Knowing how the observational units were selected from a larger entity will allow for generalizations back to the population from which the data were randomly selected.\nAdditionally, by understanding the structure of the study, causal relationships can be separated from those relationships which are only associated.\nA good question to ask oneself before working with the data at all is, \"H\n\n## Save data\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ndf = pd.DataFrame(docs, columns=['Text', 'Metadata'])\n```\n:::\n\n\n. . .\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ndf.to_csv('../docs/url/study-design.csv')\n```\n:::\n\n\n# Notion\n\n![](/images/notion.png)\n\n## Example\n\n- Option 1: Simply use the example data provided in `langchain-intro/docs/Notion_DB` \n\n- Option 2: Follow the steps [here](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/notion) for an example Notion site such as [this one](https://yolospace.notion.site/Blendle-s-Employee-Handbook-e31bff7da17346ee99f531087d8b133f)\n  - Duplicate the page into your own Notion space and export as `Markdown / CSV`.\n  - Unzip it and save it as a folder that contains the markdown file for the Notion page.\n \n## Load Notion\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nloader = NotionDirectoryLoader(\"../docs/Notion_DB\")\ndocs = loader.load()\n```\n:::\n\n\n## Inspect data\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nprint(docs[0].page_content[0:200])\n```\n:::\n\n\n```markdown\n# Getting Started\n\nüëã Welcome to Notion!\n\nHere are the basics:\n\n- [ ]  Click anywhere and just start typing\n- [ ]  Hit `/` to see all the types of content you can add - headers, videos, sub pages, etc.\n\n```\n\n## Inspect data\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ndocs[0].metadata\n```\n:::\n\n\n- {'source': '../docs/Notion_DB/Getting Started 95e5ecbe48c44e408ef09fed850fbd40.md'}\n\n\n# Acknowledgments\n\n- This tutorial is mainly based on the excellent course [\"LangChain: Chat with Your DataI\"](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/) provided by Harrison Chase from LangChain and Andrew Ng from DeepLearning.AI.\n\n\n# What's next? {background-image=\"../images/logo.png\" background-opacity=\"0.5\"}\n\n**Congratulations! You have completed this tutorial** üëç\n\n\n**Next, you may want to go back to the [lab's website](https://kirenz.github.io/lab-langchain-rag/)**\n\n",
    "supporting": [
      "01_document_loading_files"
    ],
    "filters": [],
    "includes": {}
  }
}